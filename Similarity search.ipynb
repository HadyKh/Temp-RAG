{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eef32b6b-ee49-470b-bd92-b2cbb6b8dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fa73cf-3411-4e59-972d-43945ed31c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93100dd6-feee-440a-9207-2fb37d2b982b",
   "metadata": {},
   "source": [
    "### Loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557bc0f3-b72b-4544-8839-661d8553f432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>career_level</th>\n",
       "      <th>prep_title_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Developer Relations Manager</td>\n",
       "      <td>Senior Developer Relations Manager page is loa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Senior Developer Relations Manager {title} Sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Costing Manager - Cairo</td>\n",
       "      <td>Supervise, design and implement a consistently...</td>\n",
       "      <td>&lt;ul&gt;\\n&lt;li&gt;Bachelor’s degree in Accounting&lt;/li&gt;...</td>\n",
       "      <td>Manager</td>\n",
       "      <td>Costing Manager - Cairo {title} Supervise, des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banquet Supervisor</td>\n",
       "      <td>Mandarin Oriental Hotel GroupMandarin Oriental...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experienced (Non-Manager)</td>\n",
       "      <td>Banquet Supervisor {title} Mandarin Oriental H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trade Finance &amp; Credit Collection</td>\n",
       "      <td>About Us Alfa Laval is a leading global provid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Trade Finance &amp; Credit Collection {title} Abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taste &amp; Wellbeing Creative Marketing Associate...</td>\n",
       "      <td>Join us and celebrate the beauty of human expe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Taste &amp; Wellbeing Creative Marketing Associate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0                 Senior Developer Relations Manager   \n",
       "1                            Costing Manager - Cairo   \n",
       "2                                 Banquet Supervisor   \n",
       "3                  Trade Finance & Credit Collection   \n",
       "4  Taste & Wellbeing Creative Marketing Associate...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Senior Developer Relations Manager page is loa...   \n",
       "1  Supervise, design and implement a consistently...   \n",
       "2  Mandarin Oriental Hotel GroupMandarin Oriental...   \n",
       "3  About Us Alfa Laval is a leading global provid...   \n",
       "4  Join us and celebrate the beauty of human expe...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0                                                NaN   \n",
       "1  <ul>\\n<li>Bachelor’s degree in Accounting</li>...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                career_level  \\\n",
       "0              Not specified   \n",
       "1                    Manager   \n",
       "2  Experienced (Non-Manager)   \n",
       "3              Not specified   \n",
       "4              Not specified   \n",
       "\n",
       "                              prep_title_description  \n",
       "0  Senior Developer Relations Manager {title} Sen...  \n",
       "1  Costing Manager - Cairo {title} Supervise, des...  \n",
       "2  Banquet Supervisor {title} Mandarin Oriental H...  \n",
       "3  Trade Finance & Credit Collection {title} Abou...  \n",
       "4  Taste & Wellbeing Creative Marketing Associate...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "jobs = pd.read_csv('preprocessed_jobs.csv', usecols=['job_title', 'description', 'requirements', 'career_level', 'prep_title_description'])\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "446820e7-79dc-4035-a0d7-559a1869a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "prep_title_description = list(jobs[\"prep_title_description\"])\n",
    "\n",
    "print(len(prep_title_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b6a45-3faf-40cf-b2b9-5bd9d1b50144",
   "metadata": {},
   "source": [
    "### Loading saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca45908-6566-4731-ad31-eeeeb91625a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.load('embeddings.npy')\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6797ec77-010c-4794-b945-64c7a7e91d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_length = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(embed_length)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dceb325-d1e4-48c4-ac05-0545b382f322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Check the total number of embeddings in the index\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d844070-4f92-45da-af0c-1ce1da68b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec9ad92-1a5e-494f-ab05-064f1e1c0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['Machine Learning Engineer']\n",
    "# Vectorize the query string\n",
    "query_embedding = model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f29da44-c066-4be5-acdb-0674235e2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35213 11345  5383]]\n",
      "[[0.59074473 0.612579   0.61495036]]\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Select Top k\n",
    "k = 3\n",
    "scores, index_vals = index.search(query_embedding, k)\n",
    "\n",
    "print(index_vals)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72421f52-b451-4103-912d-dd9dc632e222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Engineer {title} Showcase your software engineering talents using ML-powered profiles. Loved by 11k+ engineers! Backed by Antler.The RoleYou Will Be Responsible ForDeveloping scripts to process structured and unstructured data.Recommending, developing and implementing ways to improve data reliability, efficiency and quality.Supporting translation of data business needs into technical system requirements.Working with stakeholders to understand needs in order with respect to data structure, availability, scalability and accessibility.Developing high-quality code to build and deploy machine learning models.Ideal ProfileYou possess a degree in Computer Science, Applied Mathematics, Engineering or related field.You have at least 1 year experience, ideally within a Data Engineer role.Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.You are a strong networker & relationship builderYou pay strong attention to detail and deliver work that is of a high standardYou are a self-starter and demonstrate a high level of resilienceWhat's on Offer?Great work environmentExcellent career development opportunitiesLeadership Role\n",
      "\n",
      "AI/ Machine Learning Sr. Engineer {title} Being able to engage with the client to understand their pain points and requirements. Communicate solutions/propositions effectively back to client/team Collect and clean large datasets for machine learning projects. Explore data to identify patterns, anomalies, and potential insights. Pre-process data, including feature engineering and normalization. Design, develop, and implement machine learning algorithms. Experiment with various machine learning models and techniques. Optimize algorithms for accuracy, efficiency, and scalability. Train machine learning models using collected data. Perform testing to validate models Evaluate model performance using appropriate metrics and techniques. Fine-tune models to improve predictive accuracy. Create informative data visualizations to communicate insights effectively Familiarity with visualization libraries like Matplotlib, Seaborn, or data visualization software’s like Tableau, PowerBI. Requirements 6-10 years of experience in data engineering Essentials: Technical Expertise: Mastery in AI and ML algorithms, frameworks (e.g., TensorFlow, PyTorch), and libraries. Data proficiency: Proficient in data collection, pre-processing, and analysis. Model Mastery: Proven experience in developing and fine-tuning machine learning models. Desirable: Domain Knowledge: Familiarity with deploying ML frameworks in industry Research Enthusiast: A strong interest in staying abreast of the latest AI/ML advancements. Project Leadership: Previous experience leading AI/ML initiatives.\n",
      "\n",
      "Machine Learning Engineer {title} Siemens Digital Industries Software is a global technology powerhouse. With some of the best-known brands in the world, Siemens has stood for engineering excellence, innovation, quality, and reliability for more than 175 years.Digital Verification Technologies (DVT) division seeks 2 full-time Machine Learning Engineers to join our team in Cairo, Egypt.Responsibilities:Successful deployment of ML models in pipelines & production.Ensure using models pipeline which entails version control of models, experiments & metadata.A/B testing on various models.Optimize models for better performance, latency, memory and throughput.Monitor models performance, maintenance & support.Qualifications:B.Sc. of Computer Science or Computer Engineering or Electronics & Communications.M.Sc. in ML related fields is a plus!Strong SW Engineering background.Good Knowledge in OOP, Data Structures, Algorithms.Experience in (JAVA or C++/C).Conceptual knowledge of ML/AI methodologies.Experience in creating/optimizing data pipelines. Good knowledge of ML Frameworks like TensorFlow, PyTorch.. etc and Relational DB and NoSQL.Strong knowledge of statistics & probability theory.Highly developed communication skills, including the ability to present ideas and share your knowledge with others. We’re Siemens. A collection of over 377,000 minds building the future, one day at a time in over 200 countries. We're dedicated to equality, and we welcome applications that reflect the diversity of the communities we work in. All employment decisions at Siemens are based on qualifications, merit and business need. Bring your curiosity and creativity and help us shape tomorrow!We offer a comprehensive reward package which includes a competitive basic salary and a generous holiday allowance.Siemens is an equal opportunities employer and do not discriminate unlawfully on any grounds. We are committed to providing access, equal opportunity for individuals with disabilities in employment, its services, programs, and activities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pred_indexes in index_vals[0]:\n",
    "    print(prep_title_description[pred_indexes])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d886e1-fc40-41d7-b951-a7209426b231",
   "metadata": {},
   "source": [
    "### Decreasing response time by using Nearest Neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1e5b5ff-7c4a-437b-a34f-5f3f459e28d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing for training\n",
    "num_centroids = 5\n",
    "quantizer = faiss.IndexFlatL2(embed_length)\n",
    "index = faiss.IndexIVFFlat(quantizer, embed_length, num_centroids)\n",
    "\n",
    "index.train(embeddings) # train using our embeddings array\n",
    "\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7051b387-cb07-4faa-9875-ca814181be63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.add(embeddings) # Add the embeddings to the index\n",
    "index.ntotal # Check how many embeddings are in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f04acda-e04f-40a6-b95b-0dc163fde5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35213 11345  5383]]\n",
      "[[0.59074473 0.612579   0.61495036]]\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Select Top k\n",
    "k = 3\n",
    "scores, index_vals = index.search(query_embedding, k)\n",
    "\n",
    "print(index_vals)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34654530-69e4-432f-ba16-5d36080cee73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Engineer {title} Showcase your software engineering talents using ML-powered profiles. Loved by 11k+ engineers! Backed by Antler.The RoleYou Will Be Responsible ForDeveloping scripts to process structured and unstructured data.Recommending, developing and implementing ways to improve data reliability, efficiency and quality.Supporting translation of data business needs into technical system requirements.Working with stakeholders to understand needs in order with respect to data structure, availability, scalability and accessibility.Developing high-quality code to build and deploy machine learning models.Ideal ProfileYou possess a degree in Computer Science, Applied Mathematics, Engineering or related field.You have at least 1 year experience, ideally within a Data Engineer role.Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.You are a strong networker & relationship builderYou pay strong attention to detail and deliver work that is of a high standardYou are a self-starter and demonstrate a high level of resilienceWhat's on Offer?Great work environmentExcellent career development opportunitiesLeadership Role\n",
      "\n",
      "AI/ Machine Learning Sr. Engineer {title} Being able to engage with the client to understand their pain points and requirements. Communicate solutions/propositions effectively back to client/team Collect and clean large datasets for machine learning projects. Explore data to identify patterns, anomalies, and potential insights. Pre-process data, including feature engineering and normalization. Design, develop, and implement machine learning algorithms. Experiment with various machine learning models and techniques. Optimize algorithms for accuracy, efficiency, and scalability. Train machine learning models using collected data. Perform testing to validate models Evaluate model performance using appropriate metrics and techniques. Fine-tune models to improve predictive accuracy. Create informative data visualizations to communicate insights effectively Familiarity with visualization libraries like Matplotlib, Seaborn, or data visualization software’s like Tableau, PowerBI. Requirements 6-10 years of experience in data engineering Essentials: Technical Expertise: Mastery in AI and ML algorithms, frameworks (e.g., TensorFlow, PyTorch), and libraries. Data proficiency: Proficient in data collection, pre-processing, and analysis. Model Mastery: Proven experience in developing and fine-tuning machine learning models. Desirable: Domain Knowledge: Familiarity with deploying ML frameworks in industry Research Enthusiast: A strong interest in staying abreast of the latest AI/ML advancements. Project Leadership: Previous experience leading AI/ML initiatives.\n",
      "\n",
      "Machine Learning Engineer {title} Siemens Digital Industries Software is a global technology powerhouse. With some of the best-known brands in the world, Siemens has stood for engineering excellence, innovation, quality, and reliability for more than 175 years.Digital Verification Technologies (DVT) division seeks 2 full-time Machine Learning Engineers to join our team in Cairo, Egypt.Responsibilities:Successful deployment of ML models in pipelines & production.Ensure using models pipeline which entails version control of models, experiments & metadata.A/B testing on various models.Optimize models for better performance, latency, memory and throughput.Monitor models performance, maintenance & support.Qualifications:B.Sc. of Computer Science or Computer Engineering or Electronics & Communications.M.Sc. in ML related fields is a plus!Strong SW Engineering background.Good Knowledge in OOP, Data Structures, Algorithms.Experience in (JAVA or C++/C).Conceptual knowledge of ML/AI methodologies.Experience in creating/optimizing data pipelines. Good knowledge of ML Frameworks like TensorFlow, PyTorch.. etc and Relational DB and NoSQL.Strong knowledge of statistics & probability theory.Highly developed communication skills, including the ability to present ideas and share your knowledge with others. We’re Siemens. A collection of over 377,000 minds building the future, one day at a time in over 200 countries. We're dedicated to equality, and we welcome applications that reflect the diversity of the communities we work in. All employment decisions at Siemens are based on qualifications, merit and business need. Bring your curiosity and creativity and help us shape tomorrow!We offer a comprehensive reward package which includes a competitive basic salary and a generous holiday allowance.Siemens is an equal opportunities employer and do not discriminate unlawfully on any grounds. We are committed to providing access, equal opportunity for individuals with disabilities in employment, its services, programs, and activities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pred_indexes in index_vals[0]:\n",
    "    print(prep_title_description[pred_indexes])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c36ef3-b4b1-4f6c-823c-cf3384684463",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e801ee-f45d-41a8-b14f-b96b6149f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name = \"EleutherAI/gpt-j-6B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeeaac2-3cbe-4cb6-a90a-f2fb77079077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e20cd41-8917-4c24-a4e9-2fcd85ebdf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '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'\n",
    "OpenAI.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2345d545-beb9-40ba-9543-95c25a11f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prmpt = f\"\"\"\n",
    "You will be provided with a job title: \n",
    "{query[0]}\n",
    "Provide personal career advice basedon this job title\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "efd31e8f-524f-4546-b334-3ff94e52a86d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m----> 3\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      4\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m   messages \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:668\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    667\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    669\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    671\u001b[0m             {\n\u001b[0;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    694\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    695\u001b[0m             },\n\u001b[0;32m    696\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    697\u001b[0m         ),\n\u001b[0;32m    698\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    699\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    700\u001b[0m         ),\n\u001b[0;32m    701\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    702\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    703\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    704\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1259\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    938\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    939\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    940\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    941\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    942\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    943\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1025\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1027\u001b[0m         input_options,\n\u001b[0;32m   1028\u001b[0m         cast_to,\n\u001b[0;32m   1029\u001b[0m         retries,\n\u001b[0;32m   1030\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1031\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1032\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:1075\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1076\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1077\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1078\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1079\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1080\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1081\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1025\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1027\u001b[0m         input_options,\n\u001b[0;32m   1028\u001b[0m         cast_to,\n\u001b[0;32m   1029\u001b[0m         retries,\n\u001b[0;32m   1030\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1031\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1032\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:1075\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1076\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1077\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1078\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1079\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1080\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1081\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\w_fa_test\\Lib\\site-packages\\openai\\_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[0;32m   1050\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model = \"gpt-4o-mini\",\n",
    "  messages = prompt\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c3018-d10e-44c0-a066-dbe649ab0260",
   "metadata": {},
   "source": [
    "### integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ac1c0-1fe3-42d5-95f0-43d108aac6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_jobs(query, k=3):\n",
    "    # Vectorize the user query\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Perform a search in the FAISS index to retrieve top k job descriptions\n",
    "    scores, index_vals = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the top k job descriptions\n",
    "    top_jobs = [job_descriptions[i] for i in index_vals[0]]\n",
    "\n",
    "    print(\"Top job descriptions matching the query:\")\n",
    "    for i, job in enumerate(top_jobs):\n",
    "        print(f\"{i+1}. {job}\")\n",
    "    \n",
    "    return top_jobs\n",
    "\n",
    "def generate_recommendation(top_jobs):\n",
    "    # Create a summary of the top job descriptions\n",
    "    job_summary = \"\\n\".join(top_jobs)\n",
    "    prompt = f\"Based on the following job search results, craft a personalized job recommendation:\\n\\n{job_summary}\\n\\nRecommendation:\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=200)\n",
    "\n",
    "    # Decode and return the generated recommendation\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def process_query_and_generate_recommendation(query, k=3):\n",
    "    # Retrieve the top k job descriptions\n",
    "    top_jobs = retrieve_top_k_jobs(query, k)\n",
    "\n",
    "    # Generate a personalized recommendation based on the retrieved jobs\n",
    "    recommendation = generate_recommendation(top_jobs)\n",
    "    \n",
    "    print(\"\\nGenerated Recommendation:\")\n",
    "    print(recommendation)\n",
    "    \n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3551afb-5873-4cfe-8b75-18822d775942",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['Machine Learning Engineer']\n",
    "# Vectorize the query string\n",
    "recommendation = process_query_and_generate_recommendation(query)\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76c6b4-1a8f-4809-881e-93a5d5f0d92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (w_fa_test)",
   "language": "python",
   "name": "w_fa_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
